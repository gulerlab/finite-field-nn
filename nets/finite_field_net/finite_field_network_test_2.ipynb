{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-22T08:31:43.161380122Z",
     "start_time": "2023-06-22T08:31:42.159292756Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from networks import FiniteFieldPiNetNetworkLinear, FiniteFieldPiNetNetworkLeNet, FiniteFieldPiNetNetworkLeNetCIFAR10, FiniteFieldPiNetNetworkDebug, FiniteFieldPiNetNetworkDebug2\n",
    "from criterions import FiniteFieldMSELoss\n",
    "from datasets import load_all_data_mnist, load_all_data_cifar10\n",
    "from utils import create_batch_data, to_real_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCH = 1\n",
    "LR = 7\n",
    "PRINT = 20\n",
    "MODE = 'debug_2'\n",
    "PRIME = 2**26 - 5\n",
    "QUANTIZATION_WEIGHT = 8\n",
    "QUANTIZATION_INPUT = 8\n",
    "QUANTIZATION_BATCH_SIZE = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T08:31:43.163553631Z",
     "start_time": "2023-06-22T08:31:43.162636542Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if MODE == 1:\n",
    "    model = FiniteFieldPiNetNetworkLinear(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = True\n",
    "elif MODE == 2:\n",
    "    model = FiniteFieldPiNetNetworkLeNet(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "elif MODE == 3:\n",
    "    model = FiniteFieldPiNetNetworkLeNetCIFAR10(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "elif MODE == 'debug':\n",
    "    model = FiniteFieldPiNetNetworkDebug(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "elif MODE == 'debug_2':\n",
    "    model = FiniteFieldPiNetNetworkDebug2(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "else:\n",
    "    model = None\n",
    "    flatten = True\n",
    "criterion = FiniteFieldMSELoss(PRIME, QUANTIZATION_WEIGHT, QUANTIZATION_BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T08:31:44.458197507Z",
     "start_time": "2023-06-22T08:31:44.455555772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# data fetching\n",
    "load_path = '../../data'\n",
    "train_data, train_label, test_data, test_label = load_all_data_mnist(load_path, QUANTIZATION_INPUT, QUANTIZATION_WEIGHT, PRIME, flatten=flatten)\n",
    "train_data, train_label, test_data, test_label = create_batch_data(train_data, train_label, test_data, test_label, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T08:31:50.568427484Z",
     "start_time": "2023-06-22T08:31:45.847235085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 1, loss: 4.529284656047821\n",
      "idx: 2, loss: 344523.0065196157\n",
      "idx: 3, loss: 834972.4083711504\n",
      "idx: 4, loss: 858051.1641591787\n",
      "idx: 5, loss: 891226.0604442358\n",
      "idx: 6, loss: 868316.565542102\n",
      "idx: 7, loss: 867157.2670397162\n",
      "idx: 8, loss: 874109.4636483192\n",
      "idx: 9, loss: 856025.4221897124\n",
      "idx: 10, loss: 870294.5295047163\n",
      "idx: 11, loss: 867264.2277969718\n",
      "idx: 12, loss: 865303.8806425928\n",
      "idx: 13, loss: 853777.6647468805\n",
      "idx: 14, loss: 885227.8337913752\n",
      "idx: 15, loss: 906567.2516385316\n",
      "idx: 16, loss: 873516.5326761008\n",
      "idx: 17, loss: 881709.0769244432\n",
      "idx: 18, loss: 857226.9802350402\n",
      "idx: 19, loss: 865868.3371910453\n",
      "idx: 20, loss: 897636.7145737408\n",
      "epoch: 1, idx: 20, loss: 800938.9458460063\n",
      "epoch: 1, idx: 20, accuracy: 0.101, loss: 0.0\n",
      "idx: 21, loss: 871528.1233577133\n",
      "idx: 22, loss: 886244.2541662456\n",
      "idx: 23, loss: 894427.5418386458\n",
      "idx: 24, loss: 865978.8637493849\n",
      "idx: 25, loss: 888552.5914028882\n",
      "idx: 26, loss: 869981.2278617622\n",
      "idx: 27, loss: 842376.3674026728\n",
      "idx: 28, loss: 857729.3362460731\n",
      "idx: 29, loss: 870747.0873454213\n",
      "idx: 30, loss: 826013.1168974042\n",
      "idx: 31, loss: 874437.9493787885\n",
      "idx: 32, loss: 852053.6246388555\n",
      "idx: 33, loss: 871304.4067401886\n",
      "idx: 34, loss: 865171.1986919643\n",
      "idx: 35, loss: 868003.6946986318\n",
      "idx: 36, loss: 866394.9424338341\n",
      "idx: 37, loss: 893273.0318987967\n",
      "idx: 38, loss: 861503.2101972102\n",
      "idx: 39, loss: 887597.7051876187\n",
      "idx: 40, loss: 873685.9142469764\n",
      "epoch: 1, idx: 40, loss: 869350.2094190537\n",
      "epoch: 1, idx: 40, accuracy: 0.0989, loss: 0.0\n",
      "idx: 41, loss: 894716.6821472645\n",
      "idx: 42, loss: 874060.0306663512\n",
      "idx: 43, loss: 884500.0461341143\n",
      "idx: 44, loss: 894079.9419575332\n",
      "idx: 45, loss: 900482.8506175876\n",
      "idx: 46, loss: 871066.5143743752\n",
      "idx: 47, loss: 865137.0383292438\n",
      "idx: 48, loss: 860626.086513102\n",
      "idx: 49, loss: 870306.1524392366\n",
      "idx: 50, loss: 873901.2417109013\n",
      "idx: 51, loss: 854096.6738112569\n",
      "idx: 52, loss: 882315.1752025485\n",
      "idx: 53, loss: 890597.0911643504\n",
      "idx: 54, loss: 869984.7518174648\n",
      "idx: 55, loss: 887652.8948950171\n",
      "idx: 56, loss: 868247.4052270651\n",
      "idx: 57, loss: 844555.1283231974\n",
      "idx: 58, loss: 872757.9407800435\n",
      "idx: 59, loss: 903854.7327906489\n",
      "idx: 60, loss: 880455.7900337577\n",
      "epoch: 1, idx: 60, loss: 877169.708446753\n",
      "epoch: 1, idx: 60, accuracy: 0.0985, loss: 0.0\n",
      "idx: 61, loss: 871877.133442998\n",
      "idx: 62, loss: 878932.7693760396\n",
      "idx: 63, loss: 870603.487341106\n",
      "idx: 64, loss: 866885.877970934\n",
      "idx: 65, loss: 852908.4223340153\n",
      "idx: 66, loss: 882783.176790297\n",
      "idx: 67, loss: 870605.0266438723\n",
      "idx: 68, loss: 855957.8811126947\n",
      "idx: 69, loss: 888171.5129725934\n",
      "idx: 70, loss: 881969.9012461901\n",
      "idx: 71, loss: 871561.6271292566\n",
      "idx: 72, loss: 891447.5003325342\n",
      "idx: 73, loss: 854624.9581903219\n",
      "idx: 74, loss: 875502.8367636204\n",
      "idx: 75, loss: 879787.337372005\n",
      "idx: 76, loss: 870934.3133248688\n",
      "idx: 77, loss: 879047.2113233209\n",
      "idx: 78, loss: 895786.0095220803\n",
      "idx: 79, loss: 895139.3613091112\n",
      "idx: 80, loss: 888366.7481868864\n",
      "epoch: 1, idx: 80, loss: 876144.6546342373\n",
      "epoch: 1, idx: 80, accuracy: 0.0993, loss: 0.0\n",
      "idx: 81, loss: 873647.6534756421\n",
      "idx: 82, loss: 855672.7515718937\n",
      "idx: 83, loss: 863407.8748399615\n",
      "idx: 84, loss: 899348.9088354112\n",
      "idx: 85, loss: 860362.2767326237\n",
      "idx: 86, loss: 891067.5530557632\n",
      "idx: 87, loss: 903425.7569161057\n",
      "idx: 88, loss: 887640.5880687237\n",
      "idx: 89, loss: 868703.1641379\n",
      "idx: 90, loss: 897432.2652166486\n",
      "idx: 91, loss: 883994.217718661\n",
      "idx: 92, loss: 889366.1538270712\n",
      "idx: 93, loss: 884133.2121014\n",
      "idx: 94, loss: 862603.7196201087\n",
      "idx: 95, loss: 878342.9206203817\n",
      "idx: 96, loss: 849275.3845917581\n",
      "idx: 97, loss: 884393.8345304726\n",
      "idx: 98, loss: 868417.8055287005\n",
      "idx: 99, loss: 857170.1474311353\n",
      "idx: 100, loss: 849787.2929157614\n",
      "epoch: 1, idx: 100, loss: 875409.6740868061\n",
      "epoch: 1, idx: 100, accuracy: 0.1039, loss: 0.0\n",
      "idx: 101, loss: 858061.0068395735\n",
      "idx: 102, loss: 873468.9176348449\n",
      "idx: 103, loss: 892455.4222977163\n",
      "idx: 104, loss: 865198.1725879909\n",
      "idx: 105, loss: 901300.3017389774\n",
      "idx: 106, loss: 893321.0481781961\n",
      "idx: 107, loss: 878140.682658255\n",
      "idx: 108, loss: 855721.2395879626\n",
      "idx: 109, loss: 871364.3734819293\n",
      "idx: 110, loss: 890764.86868155\n",
      "idx: 111, loss: 863706.6408134103\n",
      "idx: 112, loss: 865110.911676705\n",
      "idx: 113, loss: 869146.1022200584\n",
      "idx: 114, loss: 890766.9263812305\n",
      "idx: 115, loss: 888348.0790550708\n",
      "idx: 116, loss: 880307.1232014298\n",
      "idx: 117, loss: 874566.3027310966\n",
      "idx: 118, loss: 867283.6594979762\n",
      "idx: 119, loss: 851340.3136316538\n",
      "idx: 120, loss: 882403.6880410312\n",
      "epoch: 1, idx: 120, loss: 875638.789046833\n",
      "epoch: 1, idx: 120, accuracy: 0.1037, loss: 0.0\n",
      "idx: 121, loss: 884037.2329468131\n",
      "idx: 122, loss: 890613.8319658041\n",
      "idx: 123, loss: 867793.8937410712\n",
      "idx: 124, loss: 885817.8218598962\n",
      "idx: 125, loss: 874079.1186517476\n",
      "idx: 126, loss: 863811.3793314697\n",
      "idx: 127, loss: 881461.2334570885\n",
      "idx: 128, loss: 865646.5005101563\n",
      "idx: 129, loss: 882620.7813487052\n",
      "idx: 130, loss: 879850.4905208944\n",
      "idx: 131, loss: 862180.6132068039\n",
      "idx: 132, loss: 882963.2861108185\n",
      "idx: 133, loss: 881545.0770002604\n",
      "idx: 134, loss: 885581.6809367539\n",
      "idx: 135, loss: 890948.4052179455\n",
      "idx: 136, loss: 872062.6697484851\n",
      "idx: 137, loss: 868160.0366613864\n",
      "idx: 138, loss: 885553.8483871223\n",
      "idx: 139, loss: 892364.5882257224\n",
      "idx: 140, loss: 873925.5013617873\n",
      "epoch: 1, idx: 140, loss: 878550.8995595366\n",
      "epoch: 1, idx: 140, accuracy: 0.0975, loss: 0.0\n",
      "idx: 141, loss: 861903.4137629865\n",
      "idx: 142, loss: 848223.5332829952\n",
      "idx: 143, loss: 854381.725728929\n",
      "idx: 144, loss: 856227.6394417286\n",
      "idx: 145, loss: 890958.886441052\n",
      "idx: 146, loss: 860084.9221366048\n",
      "idx: 147, loss: 900274.2544150351\n",
      "idx: 148, loss: 897482.3876523973\n",
      "idx: 149, loss: 873739.2998881935\n",
      "idx: 150, loss: 840733.4600260258\n",
      "idx: 151, loss: 848040.1607362032\n",
      "idx: 152, loss: 852535.4791899323\n",
      "idx: 153, loss: 863465.2519007921\n",
      "idx: 154, loss: 881065.8296610116\n",
      "idx: 155, loss: 888283.1978913546\n",
      "idx: 156, loss: 854095.3928197622\n",
      "idx: 157, loss: 866445.9207501413\n",
      "idx: 158, loss: 879921.0417371988\n",
      "idx: 159, loss: 879997.9184265734\n",
      "idx: 160, loss: 850694.9651300907\n",
      "epoch: 1, idx: 160, loss: 867427.7340509504\n",
      "epoch: 1, idx: 160, accuracy: 0.1, loss: 0.0\n",
      "idx: 161, loss: 866981.6424161196\n",
      "idx: 162, loss: 889775.6902515889\n",
      "idx: 163, loss: 870112.3141684531\n",
      "idx: 164, loss: 873497.5046479702\n",
      "idx: 165, loss: 879027.6564902067\n",
      "idx: 166, loss: 874491.9242798091\n",
      "idx: 167, loss: 889776.2831442952\n",
      "idx: 168, loss: 863253.7091810107\n",
      "idx: 169, loss: 876056.0845783948\n",
      "idx: 170, loss: 878992.4085564612\n",
      "idx: 171, loss: 908105.1665104033\n",
      "idx: 172, loss: 884568.2381753325\n",
      "idx: 173, loss: 848728.7769917846\n",
      "idx: 174, loss: 891140.0389961005\n",
      "idx: 175, loss: 878112.4120075703\n",
      "idx: 176, loss: 888446.0732232928\n",
      "idx: 177, loss: 853358.1038206816\n",
      "idx: 178, loss: 886009.6904594303\n",
      "idx: 179, loss: 857383.751315832\n",
      "idx: 180, loss: 879131.5541703106\n",
      "epoch: 1, idx: 180, loss: 876847.4511692524\n",
      "epoch: 1, idx: 180, accuracy: 0.0952, loss: 0.0\n",
      "idx: 181, loss: 863420.4982773662\n",
      "idx: 182, loss: 882956.2771641016\n",
      "idx: 183, loss: 863737.3634878397\n",
      "idx: 184, loss: 902853.5136342646\n",
      "idx: 185, loss: 892773.2540245651\n",
      "idx: 186, loss: 873810.698543191\n",
      "idx: 187, loss: 870066.742181182\n",
      "idx: 188, loss: 865895.5722309948\n",
      "idx: 189, loss: 890527.4392719269\n",
      "idx: 190, loss: 876498.9663546087\n",
      "idx: 191, loss: 858223.8575662969\n",
      "idx: 192, loss: 879856.2717514634\n",
      "idx: 193, loss: 858878.1191751958\n",
      "idx: 194, loss: 843903.3624986409\n",
      "idx: 195, loss: 855197.7626734375\n",
      "idx: 196, loss: 838065.5334061981\n",
      "idx: 197, loss: 876126.4457836151\n",
      "idx: 198, loss: 887507.0357013941\n",
      "idx: 199, loss: 866002.3609604836\n",
      "idx: 200, loss: 874213.5850797296\n",
      "epoch: 1, idx: 200, loss: 871025.7329883247\n",
      "epoch: 1, idx: 200, accuracy: 0.1025, loss: 0.0\n",
      "idx: 201, loss: 872403.8374145627\n",
      "idx: 202, loss: 871804.0659686327\n",
      "idx: 203, loss: 891436.3644728065\n",
      "idx: 204, loss: 867895.6913349628\n",
      "idx: 205, loss: 852736.7827782035\n",
      "idx: 206, loss: 880075.0973872542\n",
      "idx: 207, loss: 872334.4150621891\n",
      "idx: 208, loss: 873299.2828464508\n",
      "idx: 209, loss: 858859.3040326833\n",
      "idx: 210, loss: 870612.9692252874\n",
      "idx: 211, loss: 879396.2214096189\n",
      "idx: 212, loss: 882490.0907900333\n",
      "idx: 213, loss: 875162.7236357331\n",
      "idx: 214, loss: 900948.1794233322\n",
      "idx: 215, loss: 881419.5767367481\n",
      "idx: 216, loss: 885285.0827252269\n",
      "idx: 217, loss: 860644.8188352585\n",
      "idx: 218, loss: 839296.0674388409\n",
      "idx: 219, loss: 885771.7072564363\n",
      "idx: 220, loss: 861705.9389662741\n",
      "epoch: 1, idx: 220, loss: 873178.9108870268\n",
      "epoch: 1, idx: 220, accuracy: 0.0958, loss: 0.0\n",
      "idx: 221, loss: 891551.6880941987\n",
      "idx: 222, loss: 859347.4714648725\n",
      "idx: 223, loss: 881606.3371780515\n",
      "idx: 224, loss: 843907.528637767\n",
      "idx: 225, loss: 873708.2045419216\n",
      "idx: 226, loss: 882179.2260106802\n",
      "idx: 227, loss: 844779.7601801156\n",
      "idx: 228, loss: 904780.7566316128\n",
      "idx: 229, loss: 858939.4435350895\n",
      "idx: 230, loss: 872250.938223958\n",
      "idx: 231, loss: 886679.2352339627\n",
      "idx: 232, loss: 869629.3619094491\n",
      "idx: 233, loss: 860155.8638417125\n",
      "idx: 234, loss: 893228.4880513549\n",
      "idx: 235, loss: 853521.2691771189\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    tot_loss = 0\n",
    "    for train_idx, (train_data_batch, train_label_batch) in enumerate(zip(train_data, train_label)):\n",
    "        # train\n",
    "        preds = model.forward(train_data_batch)\n",
    "\n",
    "        loss = criterion.forward(preds, train_label_batch)\n",
    "        tot_loss += loss\n",
    "        propagated_error = criterion.error_derivative()\n",
    "\n",
    "        model.backprop(propagated_error)\n",
    "        model.optimize(LR)\n",
    "        print('idx: {}, loss: {}'.format(train_idx + 1, loss))\n",
    "        if (train_idx + 1) % PRINT == 0:\n",
    "            print('epoch: {}, idx: {}, loss: {}'.format(epoch + 1, train_idx + 1, tot_loss / PRINT))\n",
    "            tot_loss = 0\n",
    "\n",
    "        if (train_idx + 1) % PRINT == 0:\n",
    "            tot_acc = 0\n",
    "            tot_sample = 0\n",
    "            for train_acc_idx, (test_data_batch, test_label_batch) in enumerate(zip(test_data, test_label)):\n",
    "                # train accuracy\n",
    "                preds = model.forward(test_data_batch)\n",
    "                real_preds = to_real_domain(preds, QUANTIZATION_WEIGHT, PRIME)\n",
    "                pred_args = np.argmax(real_preds, axis=1)\n",
    "\n",
    "                tot_acc += np.count_nonzero(pred_args == test_label_batch)\n",
    "                tot_sample += test_data_batch.shape[0]\n",
    "\n",
    "            accuracy = tot_acc / tot_sample\n",
    "            if train_idx != 0:\n",
    "                tot_loss = tot_loss / PRINT\n",
    "            print('epoch: {}, idx: {}, accuracy: {}, loss: {}'.format(epoch + 1, train_idx + 1, accuracy, tot_loss))\n",
    "            tot_loss = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
