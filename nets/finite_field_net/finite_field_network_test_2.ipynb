{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-22T08:31:43.161380122Z",
     "start_time": "2023-06-22T08:31:42.159292756Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from networks import FiniteFieldPiNetNetworkLinear, FiniteFieldPiNetNetworkLeNet, FiniteFieldPiNetNetworkLeNetCIFAR10, FiniteFieldPiNetNetworkDebug, FiniteFieldPiNetNetworkDebug2\n",
    "from criterions import FiniteFieldMSELoss\n",
    "from datasets import load_all_data_mnist, load_all_data_cifar10\n",
    "from utils import create_batch_data, to_real_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCH = 1\n",
    "LR = 7\n",
    "PRINT = 20\n",
    "MODE = 'debug_2'\n",
    "PRIME = 2**26 - 5\n",
    "QUANTIZATION_WEIGHT = 8\n",
    "QUANTIZATION_INPUT = 8\n",
    "QUANTIZATION_BATCH_SIZE = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T08:31:43.163553631Z",
     "start_time": "2023-06-22T08:31:43.162636542Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if MODE == 1:\n",
    "    model = FiniteFieldPiNetNetworkLinear(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = True\n",
    "elif MODE == 2:\n",
    "    model = FiniteFieldPiNetNetworkLeNet(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "elif MODE == 3:\n",
    "    model = FiniteFieldPiNetNetworkLeNetCIFAR10(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "elif MODE == 'debug':\n",
    "    model = FiniteFieldPiNetNetworkDebug(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "elif MODE == 'debug_2':\n",
    "    model = FiniteFieldPiNetNetworkDebug2(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "else:\n",
    "    model = None\n",
    "    flatten = True\n",
    "criterion = FiniteFieldMSELoss(PRIME, QUANTIZATION_WEIGHT, QUANTIZATION_BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T08:31:44.458197507Z",
     "start_time": "2023-06-22T08:31:44.455555772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# data fetching\n",
    "load_path = '../../data'\n",
    "train_data, train_label, test_data, test_label = load_all_data_mnist(load_path, QUANTIZATION_INPUT, QUANTIZATION_WEIGHT, PRIME, flatten=flatten)\n",
    "train_data, train_label, test_data, test_label = create_batch_data(train_data, train_label, test_data, test_label, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T08:31:50.568427484Z",
     "start_time": "2023-06-22T08:31:45.847235085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 1, loss: 4.529284656047821\n",
      "idx: 2, loss: 344523.0065196157\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    tot_loss = 0\n",
    "    for train_idx, (train_data_batch, train_label_batch) in enumerate(zip(train_data, train_label)):\n",
    "        # train\n",
    "        preds = model.forward(train_data_batch)\n",
    "\n",
    "        loss = criterion.forward(preds, train_label_batch)\n",
    "        tot_loss += loss\n",
    "        propagated_error = criterion.error_derivative()\n",
    "\n",
    "        model.backprop(propagated_error)\n",
    "        model.optimize(LR)\n",
    "        print('idx: {}, loss: {}'.format(train_idx + 1, loss))\n",
    "        if (train_idx + 1) % PRINT == 0:\n",
    "            print('epoch: {}, idx: {}, loss: {}'.format(epoch + 1, train_idx + 1, tot_loss / PRINT))\n",
    "            tot_loss = 0\n",
    "\n",
    "        if (train_idx + 1) % PRINT == 0:\n",
    "            tot_acc = 0\n",
    "            tot_sample = 0\n",
    "            for train_acc_idx, (test_data_batch, test_label_batch) in enumerate(zip(test_data, test_label)):\n",
    "                # train accuracy\n",
    "                preds = model.forward(test_data_batch)\n",
    "                real_preds = to_real_domain(preds, QUANTIZATION_WEIGHT, PRIME)\n",
    "                pred_args = np.argmax(real_preds, axis=1)\n",
    "\n",
    "                tot_acc += np.count_nonzero(pred_args == test_label_batch)\n",
    "                tot_sample += test_data_batch.shape[0]\n",
    "\n",
    "            accuracy = tot_acc / tot_sample\n",
    "            if train_idx != 0:\n",
    "                tot_loss = tot_loss / PRINT\n",
    "            print('epoch: {}, idx: {}, accuracy: {}, loss: {}'.format(epoch + 1, train_idx + 1, accuracy, tot_loss))\n",
    "            tot_loss = 0"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
