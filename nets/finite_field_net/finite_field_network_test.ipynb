{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-22T04:14:30.905220786Z",
     "start_time": "2023-06-22T04:14:30.243296544Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from networks import FiniteFieldPiNetNetworkLinear, FiniteFieldPiNetNetworkLeNet, FiniteFieldPiNetNetworkLeNetCIFAR10, FiniteFieldPiNetNetworkDebug\n",
    "from criterions import FiniteFieldMSELoss\n",
    "from datasets import load_all_data_mnist, load_all_data_cifar10\n",
    "from utils import create_batch_data, to_real_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCH = 1\n",
    "LR = 7\n",
    "PRINT = 10\n",
    "MODE = 'debug'\n",
    "PRIME = 2**26 - 5\n",
    "QUANTIZATION_WEIGHT = 8\n",
    "QUANTIZATION_INPUT = 8\n",
    "QUANTIZATION_BATCH_SIZE = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T04:14:31.871373531Z",
     "start_time": "2023-06-22T04:14:31.870270927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if MODE == 1:\n",
    "    model = FiniteFieldPiNetNetworkLinear(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = True\n",
    "elif MODE == 2:\n",
    "    model = FiniteFieldPiNetNetworkLeNet(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "elif MODE == 3:\n",
    "    model = FiniteFieldPiNetNetworkLeNetCIFAR10(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "elif MODE == 'debug':\n",
    "    model = FiniteFieldPiNetNetworkDebug(QUANTIZATION_WEIGHT, PRIME, QUANTIZATION_INPUT)\n",
    "    flatten = False\n",
    "else:\n",
    "    model = None\n",
    "    flatten = True\n",
    "criterion = FiniteFieldMSELoss(PRIME, QUANTIZATION_WEIGHT, QUANTIZATION_BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T04:14:33.261212601Z",
     "start_time": "2023-06-22T04:14:33.234422195Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# data fetching\n",
    "load_path = '../../data'\n",
    "train_data, train_label, test_data, test_label = load_all_data_mnist(load_path, QUANTIZATION_INPUT, QUANTIZATION_WEIGHT, PRIME, flatten=flatten)\n",
    "train_data, train_label, test_data, test_label = create_batch_data(train_data, train_label, test_data, test_label, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T04:14:39.062417296Z",
     "start_time": "2023-06-22T04:14:34.548524123Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 1, loss: 2.8752511739730835\n",
      "idx: 2, loss: 1.820149004459381\n",
      "idx: 3, loss: 3.1006760597229\n",
      "idx: 4, loss: 2.352128386497498\n",
      "idx: 5, loss: 1.8502511978149414\n",
      "idx: 6, loss: 0.9645739197731018\n",
      "idx: 7, loss: 0.8610182404518127\n",
      "idx: 8, loss: 0.9194228649139405\n",
      "idx: 9, loss: 0.805298924446106\n",
      "idx: 10, loss: 0.7365047931671143\n",
      "epoch: 1, idx: 10, loss: 1.6285274565219878\n",
      "epoch: 1, idx: 10, accuracy: 0.5838, loss: 0.0\n",
      "idx: 11, loss: 0.7790211439132689\n",
      "idx: 12, loss: 0.8088868856430053\n",
      "idx: 13, loss: 0.7293991446495056\n",
      "idx: 14, loss: 0.811723232269287\n",
      "idx: 15, loss: 0.7457399964332582\n",
      "idx: 16, loss: 0.6620576977729797\n",
      "idx: 17, loss: 0.6312770843505859\n",
      "idx: 18, loss: 0.6187778115272522\n",
      "idx: 19, loss: 0.6908708214759827\n",
      "idx: 20, loss: 0.615646719932556\n",
      "epoch: 1, idx: 20, loss: 0.7093400537967682\n",
      "epoch: 1, idx: 20, accuracy: 0.7135, loss: 0.0\n",
      "idx: 21, loss: 0.598652720451355\n",
      "idx: 22, loss: 0.5760615468025208\n",
      "idx: 23, loss: 0.5898118615150453\n",
      "idx: 24, loss: 0.6099125146865845\n",
      "idx: 25, loss: 0.6280621886253356\n",
      "idx: 26, loss: 0.5589463710784912\n",
      "idx: 27, loss: 0.6182720065116882\n",
      "idx: 28, loss: 0.6071428060531616\n",
      "idx: 29, loss: 0.6685732007026672\n",
      "idx: 30, loss: 0.5880357623100281\n",
      "epoch: 1, idx: 30, loss: 0.6043470978736878\n",
      "epoch: 1, idx: 30, accuracy: 0.7611, loss: 0.0\n",
      "idx: 31, loss: 0.6079101562500001\n",
      "idx: 32, loss: 0.6081599593162536\n",
      "idx: 33, loss: 0.6242583990097046\n",
      "idx: 34, loss: 0.5982872843742371\n",
      "idx: 35, loss: 0.6302079558372496\n",
      "idx: 36, loss: 0.541654646396637\n",
      "idx: 37, loss: 0.5927634835243226\n",
      "idx: 38, loss: 0.5750876069068909\n",
      "idx: 39, loss: 0.5508901476860046\n",
      "idx: 40, loss: 0.5760064721107483\n",
      "epoch: 1, idx: 40, loss: 0.5905226111412049\n",
      "epoch: 1, idx: 40, accuracy: 0.7693, loss: 0.0\n",
      "idx: 41, loss: 0.5569904446601868\n",
      "idx: 42, loss: 0.5246884226799011\n",
      "idx: 43, loss: 0.5191954374313354\n",
      "idx: 44, loss: 0.5297201871871948\n",
      "idx: 45, loss: 0.48127508163452154\n",
      "idx: 46, loss: 0.5550262928009033\n",
      "idx: 47, loss: 0.5163716077804565\n",
      "idx: 48, loss: 0.48762357234954834\n",
      "idx: 49, loss: 0.5449659824371338\n",
      "idx: 50, loss: 0.5931259393692017\n",
      "epoch: 1, idx: 50, loss: 0.5308982968330384\n",
      "epoch: 1, idx: 50, accuracy: 0.8032, loss: 0.0\n",
      "idx: 51, loss: 0.563427209854126\n",
      "idx: 52, loss: 0.607689142227173\n",
      "idx: 53, loss: 0.6018899083137512\n",
      "idx: 54, loss: 0.5903852581977844\n",
      "idx: 55, loss: 0.8196443915367128\n",
      "idx: 56, loss: 0.8176472783088684\n",
      "idx: 57, loss: 0.6920723319053649\n",
      "idx: 58, loss: 0.7000260353088378\n",
      "idx: 59, loss: 0.4758905172348023\n",
      "idx: 60, loss: 0.5129207968711853\n",
      "epoch: 1, idx: 60, loss: 0.6381592869758606\n",
      "epoch: 1, idx: 60, accuracy: 0.7706, loss: 0.0\n",
      "idx: 61, loss: 0.5391921997070312\n",
      "idx: 62, loss: 0.6136918067932129\n",
      "idx: 63, loss: 0.6044924855232238\n",
      "idx: 64, loss: 0.5124977231025697\n",
      "idx: 65, loss: 0.46481233835220337\n",
      "idx: 66, loss: 0.5637198686599731\n",
      "idx: 67, loss: 0.6282253861427306\n",
      "idx: 68, loss: 0.7173956632614135\n",
      "idx: 69, loss: 0.6239007711410522\n",
      "idx: 70, loss: 0.5485770106315613\n",
      "epoch: 1, idx: 70, loss: 0.5816505253314972\n",
      "epoch: 1, idx: 70, accuracy: 0.8208, loss: 0.0\n",
      "idx: 71, loss: 0.4773260354995727\n",
      "idx: 72, loss: 0.4912422299385071\n",
      "idx: 73, loss: 0.5103787183761597\n",
      "idx: 74, loss: 0.5193599462509155\n",
      "idx: 75, loss: 0.5261843800544739\n",
      "idx: 76, loss: 0.5932023525238038\n",
      "idx: 77, loss: 0.7036336660385131\n",
      "idx: 78, loss: 1.1093596816062925\n",
      "idx: 79, loss: 1.8952658772468565\n",
      "idx: 80, loss: 0.6414190530776978\n",
      "epoch: 1, idx: 80, loss: 0.7467371940612793\n",
      "epoch: 1, idx: 80, accuracy: 0.7656, loss: 0.0\n",
      "idx: 81, loss: 0.5031187534332274\n",
      "idx: 82, loss: 0.5457720756530762\n",
      "idx: 83, loss: 0.4943026900291443\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    tot_loss = 0\n",
    "    for train_idx, (train_data_batch, train_label_batch) in enumerate(zip(train_data, train_label)):\n",
    "        # train\n",
    "        preds = model.forward(train_data_batch)\n",
    "\n",
    "        loss = criterion.forward(preds, train_label_batch)\n",
    "        tot_loss += loss\n",
    "        propagated_error = criterion.error_derivative()\n",
    "\n",
    "        model.backprop(propagated_error)\n",
    "        model.optimize(LR)\n",
    "        print('idx: {}, loss: {}'.format(train_idx + 1, loss))\n",
    "        if (train_idx + 1) % PRINT == 0:\n",
    "            print('epoch: {}, idx: {}, loss: {}'.format(epoch + 1, train_idx + 1, tot_loss / PRINT))\n",
    "            tot_loss = 0\n",
    "\n",
    "        if (train_idx + 1) % PRINT == 0:\n",
    "            tot_acc = 0\n",
    "            tot_sample = 0\n",
    "            for train_acc_idx, (test_data_batch, test_label_batch) in enumerate(zip(test_data, test_label)):\n",
    "                # train accuracy\n",
    "                preds = model.forward(test_data_batch)\n",
    "                real_preds = to_real_domain(preds, QUANTIZATION_WEIGHT, PRIME)\n",
    "                pred_args = np.argmax(real_preds, axis=1)\n",
    "\n",
    "                tot_acc += np.count_nonzero(pred_args == test_label_batch)\n",
    "                tot_sample += test_data_batch.shape[0]\n",
    "\n",
    "            accuracy = tot_acc / tot_sample\n",
    "            if train_idx != 0:\n",
    "                tot_loss = tot_loss / PRINT\n",
    "            print('epoch: {}, idx: {}, accuracy: {}, loss: {}'.format(epoch + 1, train_idx + 1, accuracy, tot_loss))\n",
    "            tot_loss = 0"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
