{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-23T02:23:24.291017906Z",
     "start_time": "2023-06-23T02:23:23.459488172Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umityigitbsrn/miniconda3/envs/pytorch-stable/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_all_data_mnist, load_all_data_cifar10, load_all_data_fashion_mnist, load_all_data_apply_vgg_cifar10\n",
    "from utils import create_batch_data, to_finite_field_domain, to_real_domain\n",
    "import modules\n",
    "import layers\n",
    "from criterions import FiniteFieldMSELoss\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCH = 100\n",
    "PRINT = 1\n",
    "FLATTEN = False\n",
    "# 0, MNIST; 1, FashionMNIST; 2, CIFAR10; 3, VGG-CIFAR10; 4 RANDOM\n",
    "DATASET_MODE = 4\n",
    "PRIME = 2**26 - 5\n",
    "QUANTIZATION_WEIGHT = 8\n",
    "QUANTIZATION_INPUT = 8\n",
    "QUANTIZATION_BATCH_SIZE = 8\n",
    "LR = 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T02:23:24.352035118Z",
     "start_time": "2023-06-23T02:23:24.351754172Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# data fetching\n",
    "load_path = '../../data'\n",
    "if DATASET_MODE == 0:\n",
    "    train_data, train_label, test_data, test_label = load_all_data_mnist(load_path, QUANTIZATION_INPUT, QUANTIZATION_WEIGHT, PRIME, flatten=FLATTEN)\n",
    "elif DATASET_MODE == 1:\n",
    "    train_data, train_label, test_data, test_label = load_all_data_fashion_mnist(load_path, QUANTIZATION_INPUT, QUANTIZATION_WEIGHT, PRIME, flatten=FLATTEN)\n",
    "elif DATASET_MODE == 2:\n",
    "    train_data, train_label, test_data, test_label = load_all_data_cifar10(load_path, QUANTIZATION_INPUT, QUANTIZATION_WEIGHT, PRIME, flatten=FLATTEN)\n",
    "elif DATASET_MODE == 3:\n",
    "    train_data, train_label, test_data, test_label = load_all_data_apply_vgg_cifar10(load_path, QUANTIZATION_INPUT, QUANTIZATION_WEIGHT, PRIME, flatten=FLATTEN)\n",
    "elif DATASET_MODE == 4:\n",
    "    train_data, train_label = make_classification(n_samples=25, n_features=25, n_classes=10, n_clusters_per_class=1, n_informative=5)\n",
    "    train_data = train_data.reshape((-1, 5, 5))[:, np.newaxis, :, :]\n",
    "    test_data, test_label = train_data, train_label\n",
    "    train_label = np.zeros((25, 10))\n",
    "    for idx, label in enumerate(test_label):\n",
    "        train_label[idx][label] = 1\n",
    "    train_data, train_label, test_data = to_finite_field_domain(train_data, QUANTIZATION_INPUT, PRIME), to_finite_field_domain(train_label, QUANTIZATION_WEIGHT, PRIME), to_finite_field_domain(test_data, QUANTIZATION_INPUT, PRIME)\n",
    "else:\n",
    "    train_data, train_label, test_data, test_label = None, None, None, None\n",
    "train_data, train_label, test_data, test_label = create_batch_data(train_data, train_label, test_data, test_label, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T02:23:38.034717327Z",
     "start_time": "2023-06-23T02:23:38.032017982Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model_arr = [\n",
    "    layers.FiniteFieldPiNetSecondOrderConvLayer(1, 3, (2, 2), QUANTIZATION_WEIGHT, PRIME, first_layer=True, quantization_bit_input=QUANTIZATION_INPUT),\n",
    "    layers.FiniteFieldPiNetSecondOrderConvLayer(3, 3, (2, 2), QUANTIZATION_WEIGHT, PRIME),\n",
    "    modules.Flatten(),\n",
    "    layers.FiniteFieldPiNetSecondOrderLinearLayer(27, 10, QUANTIZATION_WEIGHT, PRIME),\n",
    "    layers.FiniteFieldLinearLayer(10, 10, QUANTIZATION_WEIGHT, PRIME)\n",
    "]\n",
    "\n",
    "model = modules.Network(model_arr)\n",
    "criterion = FiniteFieldMSELoss(PRIME, QUANTIZATION_WEIGHT, QUANTIZATION_BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T02:23:38.942384912Z",
     "start_time": "2023-06-23T02:23:38.937744805Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, idx: 1, accuracy: 0.08, loss: 1.1233520507812502\n",
      "epoch: 2, idx: 1, accuracy: 0.12, loss: 1.1164111328125\n",
      "epoch: 3, idx: 1, accuracy: 0.12, loss: 1.1125518798828125\n",
      "epoch: 4, idx: 1, accuracy: 0.08, loss: 1.1082269287109374\n",
      "epoch: 5, idx: 1, accuracy: 0.12, loss: 1.1010241699218748\n",
      "epoch: 6, idx: 1, accuracy: 0.12, loss: 1.0968377685546875\n",
      "epoch: 7, idx: 1, accuracy: 0.12, loss: 1.0941638183593752\n",
      "epoch: 8, idx: 1, accuracy: 0.12, loss: 1.0857434082031248\n",
      "epoch: 9, idx: 1, accuracy: 0.12, loss: 1.0846063232421874\n",
      "epoch: 10, idx: 1, accuracy: 0.12, loss: 1.0777478027343748\n",
      "epoch: 11, idx: 1, accuracy: 0.12, loss: 1.0740057373046874\n",
      "epoch: 12, idx: 1, accuracy: 0.12, loss: 1.0692596435546873\n",
      "epoch: 13, idx: 1, accuracy: 0.12, loss: 1.0637530517578124\n",
      "epoch: 14, idx: 1, accuracy: 0.12, loss: 1.0622784423828124\n",
      "epoch: 15, idx: 1, accuracy: 0.12, loss: 1.0561614990234376\n",
      "epoch: 16, idx: 1, accuracy: 0.12, loss: 1.055255126953125\n",
      "epoch: 17, idx: 1, accuracy: 0.12, loss: 1.05571044921875\n",
      "epoch: 18, idx: 1, accuracy: 0.12, loss: 1.05237060546875\n",
      "epoch: 19, idx: 1, accuracy: 0.12, loss: 1.0504461669921874\n",
      "epoch: 20, idx: 1, accuracy: 0.12, loss: 1.0477166748046873\n",
      "epoch: 21, idx: 1, accuracy: 0.12, loss: 1.0417687988281252\n",
      "epoch: 22, idx: 1, accuracy: 0.12, loss: 1.036846923828125\n",
      "epoch: 23, idx: 1, accuracy: 0.12, loss: 1.0329510498046874\n",
      "epoch: 24, idx: 1, accuracy: 0.12, loss: 1.0294842529296875\n",
      "epoch: 25, idx: 1, accuracy: 0.12, loss: 1.0257293701171875\n",
      "epoch: 26, idx: 1, accuracy: 0.12, loss: 1.0217510986328124\n",
      "epoch: 27, idx: 1, accuracy: 0.12, loss: 1.0189630126953126\n",
      "epoch: 28, idx: 1, accuracy: 0.12, loss: 1.016929931640625\n",
      "epoch: 29, idx: 1, accuracy: 0.12, loss: 1.0155078125\n",
      "epoch: 30, idx: 1, accuracy: 0.12, loss: 1.01028564453125\n",
      "epoch: 31, idx: 1, accuracy: 0.12, loss: 1.0102606201171875\n",
      "epoch: 32, idx: 1, accuracy: 0.12, loss: 1.0078057861328127\n",
      "epoch: 33, idx: 1, accuracy: 0.12, loss: 1.0061840820312498\n",
      "epoch: 34, idx: 1, accuracy: 0.12, loss: 1.0033990478515624\n",
      "epoch: 35, idx: 1, accuracy: 0.12, loss: 1.0035638427734375\n",
      "epoch: 36, idx: 1, accuracy: 0.12, loss: 1.0010650634765625\n",
      "epoch: 37, idx: 1, accuracy: 0.12, loss: 1.0002978515625\n",
      "epoch: 38, idx: 1, accuracy: 0.12, loss: 0.9979534912109375\n",
      "epoch: 39, idx: 1, accuracy: 0.12, loss: 0.9959442138671873\n",
      "epoch: 40, idx: 1, accuracy: 0.12, loss: 0.9930255126953125\n",
      "epoch: 41, idx: 1, accuracy: 0.12, loss: 0.9886602783203127\n",
      "epoch: 42, idx: 1, accuracy: 0.16, loss: 0.98607421875\n",
      "epoch: 43, idx: 1, accuracy: 0.16, loss: 0.9855432128906249\n",
      "epoch: 44, idx: 1, accuracy: 0.16, loss: 0.9826330566406248\n",
      "epoch: 45, idx: 1, accuracy: 0.16, loss: 0.9810375976562499\n",
      "epoch: 46, idx: 1, accuracy: 0.16, loss: 0.9793170166015623\n",
      "epoch: 47, idx: 1, accuracy: 0.16, loss: 0.9763085937500001\n",
      "epoch: 48, idx: 1, accuracy: 0.16, loss: 0.974552001953125\n",
      "epoch: 49, idx: 1, accuracy: 0.16, loss: 0.9721118164062501\n",
      "epoch: 50, idx: 1, accuracy: 0.16, loss: 0.9711199951171875\n",
      "epoch: 51, idx: 1, accuracy: 0.16, loss: 0.9702783203124998\n",
      "epoch: 52, idx: 1, accuracy: 0.16, loss: 0.9702539062500002\n",
      "epoch: 53, idx: 1, accuracy: 0.16, loss: 0.9706951904296877\n",
      "epoch: 54, idx: 1, accuracy: 0.16, loss: 0.9685931396484374\n",
      "epoch: 55, idx: 1, accuracy: 0.12, loss: 0.9673773193359375\n",
      "epoch: 56, idx: 1, accuracy: 0.16, loss: 0.96689453125\n",
      "epoch: 57, idx: 1, accuracy: 0.16, loss: 0.9651239013671874\n",
      "epoch: 58, idx: 1, accuracy: 0.16, loss: 0.9636724853515625\n",
      "epoch: 59, idx: 1, accuracy: 0.16, loss: 0.9620758056640625\n",
      "epoch: 60, idx: 1, accuracy: 0.16, loss: 0.9610900878906249\n",
      "epoch: 61, idx: 1, accuracy: 0.16, loss: 0.9593743896484374\n",
      "epoch: 62, idx: 1, accuracy: 0.16, loss: 0.958193359375\n",
      "epoch: 63, idx: 1, accuracy: 0.16, loss: 0.9548876953125002\n",
      "epoch: 64, idx: 1, accuracy: 0.16, loss: 0.956109619140625\n",
      "epoch: 65, idx: 1, accuracy: 0.16, loss: 0.9547576904296875\n",
      "epoch: 66, idx: 1, accuracy: 0.2, loss: 0.95377197265625\n",
      "epoch: 67, idx: 1, accuracy: 0.16, loss: 0.9528509521484375\n",
      "epoch: 68, idx: 1, accuracy: 0.16, loss: 0.94934814453125\n",
      "epoch: 69, idx: 1, accuracy: 0.2, loss: 0.9493017578125001\n",
      "epoch: 70, idx: 1, accuracy: 0.2, loss: 0.9438287353515626\n",
      "epoch: 71, idx: 1, accuracy: 0.2, loss: 0.9424682617187501\n",
      "epoch: 72, idx: 1, accuracy: 0.2, loss: 0.9414166259765625\n",
      "epoch: 73, idx: 1, accuracy: 0.16, loss: 0.9404327392578125\n",
      "epoch: 74, idx: 1, accuracy: 0.2, loss: 0.936634521484375\n",
      "epoch: 75, idx: 1, accuracy: 0.2, loss: 0.9376770019531251\n",
      "epoch: 76, idx: 1, accuracy: 0.16, loss: 0.9329980468750001\n",
      "epoch: 77, idx: 1, accuracy: 0.16, loss: 0.9354962158203125\n",
      "epoch: 78, idx: 1, accuracy: 0.16, loss: 0.933939208984375\n",
      "epoch: 79, idx: 1, accuracy: 0.16, loss: 0.9342968750000001\n",
      "epoch: 80, idx: 1, accuracy: 0.16, loss: 0.9329248046875002\n",
      "epoch: 81, idx: 1, accuracy: 0.16, loss: 0.932698974609375\n",
      "epoch: 82, idx: 1, accuracy: 0.16, loss: 0.9326861572265627\n",
      "epoch: 83, idx: 1, accuracy: 0.16, loss: 0.9301678466796877\n",
      "epoch: 84, idx: 1, accuracy: 0.16, loss: 0.9293151855468749\n",
      "epoch: 85, idx: 1, accuracy: 0.24, loss: 0.929132080078125\n",
      "epoch: 86, idx: 1, accuracy: 0.2, loss: 0.929163818359375\n",
      "epoch: 87, idx: 1, accuracy: 0.24, loss: 0.9276037597656248\n",
      "epoch: 88, idx: 1, accuracy: 0.24, loss: 0.9254766845703126\n",
      "epoch: 89, idx: 1, accuracy: 0.16, loss: 0.9255133056640623\n",
      "epoch: 90, idx: 1, accuracy: 0.28, loss: 0.9248583984375\n",
      "epoch: 91, idx: 1, accuracy: 0.24, loss: 0.9227526855468748\n",
      "epoch: 92, idx: 1, accuracy: 0.24, loss: 0.92189453125\n",
      "epoch: 93, idx: 1, accuracy: 0.28, loss: 0.920078125\n",
      "epoch: 94, idx: 1, accuracy: 0.24, loss: 0.9179229736328125\n",
      "epoch: 95, idx: 1, accuracy: 0.24, loss: 0.915928955078125\n",
      "epoch: 96, idx: 1, accuracy: 0.24, loss: 0.9165930175781252\n",
      "epoch: 97, idx: 1, accuracy: 0.24, loss: 0.9151568603515626\n",
      "epoch: 98, idx: 1, accuracy: 0.24, loss: 0.9148931884765623\n",
      "epoch: 99, idx: 1, accuracy: 0.28, loss: 0.913135986328125\n",
      "epoch: 100, idx: 1, accuracy: 0.28, loss: 0.914429931640625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    tot_loss = 0\n",
    "    for train_idx, (train_data_batch, train_label_batch) in enumerate(zip(train_data, train_label)):\n",
    "        # train\n",
    "        preds = model.forward(train_data_batch)\n",
    "\n",
    "        loss = criterion.forward(preds, train_label_batch)\n",
    "        tot_loss += loss\n",
    "        propagated_error = criterion.error_derivative()\n",
    "\n",
    "        model.backprop(propagated_error)\n",
    "        model.optimize(LR)\n",
    "\n",
    "        if (train_idx + 1) % PRINT == 0:\n",
    "            tot_acc = 0\n",
    "            tot_sample = 0\n",
    "            for train_acc_idx, (test_data_batch, test_label_batch) in enumerate(zip(test_data, test_label)):\n",
    "                # train accuracy\n",
    "                preds = model.forward(test_data_batch)\n",
    "                preds = to_real_domain(preds, QUANTIZATION_WEIGHT, PRIME)\n",
    "                pred_args = np.argmax(preds, axis=1)\n",
    "\n",
    "                tot_acc += np.count_nonzero(pred_args == test_label_batch)\n",
    "                tot_sample += test_data_batch.shape[0]\n",
    "            accuracy = tot_acc / tot_sample\n",
    "            if train_idx != 0:\n",
    "                tot_loss = tot_loss / PRINT\n",
    "            print('epoch: {}, idx: {}, accuracy: {}, loss: {}'.format(epoch + 1, train_idx + 1, accuracy, tot_loss))\n",
    "            tot_loss = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T02:23:58.908548661Z",
     "start_time": "2023-06-23T02:23:39.786927805Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}